{"cells":[{"cell_type":"markdown","source":["# Web Scraper for gov.uk Immigration Legislation\n","\n","This Python script is designed to scrape content from the UK government's official website (gov.uk), specifically focusing on immigration legislation and related information. It utilizes the Selenium library for web browsing automation and BeautifulSoup for HTML parsing. The data extracted includes URLs, titles, and textual content from various pages, which is then stored in a CSV file."],"metadata":{"id":"nOttfH8clB8q"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"UOCaMBE7k3Hi"},"outputs":[],"source":["import csv\n","import logging\n","from selenium import webdriver\n","from selenium.webdriver.firefox.service import Service\n","from selenium.webdriver.firefox.options import Options\n","from bs4 import BeautifulSoup\n","import time"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MDWsZnixk3Hk"},"outputs":[],"source":["def scrape_category(category_url, category_title, csvwriter, visited_urls):\n","    if category_url in visited_urls:\n","        return\n","    visited_urls.add(category_url)\n","\n","    logging.info(f\"Visiting category URL: {category_url}\")\n","\n","    try:\n","        driver.get(category_url)\n","        time.sleep(5)  # Increase wait time to ensure the page fully loads\n","\n","        # Parse the page with BeautifulSoup\n","        page_soup = BeautifulSoup(driver.page_source, 'html.parser')\n","        content = page_soup.find('div', id='wrapper', class_='wrapper')\n","\n","        if not content:\n","            logging.error(f\"No content found on {category_url}\")\n","            return\n","\n","        # Extract all paragraphs\n","        paragraphs = ' '.join([p.get_text(strip=True) for p in content.find_all('p')])\n","\n","        # Write the category content to the CSV\n","        csvwriter.writerow([category_url, category_title, paragraphs])\n","        logging.info(f\"Successfully scraped category URL: {category_url}\")\n","\n","        # Find and follow sub-links within the category\n","        sub_links = content.find_all('a', href=True)\n","        for sub_link in sub_links:\n","            sub_href = sub_link['href']\n","            if sub_href.startswith('/'):\n","                sub_full_url = \"https://www.gov.uk\" + sub_href\n","                scrape_section(sub_full_url, category_title, csvwriter, visited_urls)\n","            elif sub_href.startswith('http'):\n","                scrape_section(sub_href, category_title, csvwriter, visited_urls)\n","\n","    except Exception as e:\n","        logging.error(f\"An error occurred while scraping category {category_url}: {e}\")\n","\n","def scrape_section(section_url, category_title, csvwriter, visited_urls):\n","    if section_url in visited_urls:\n","        return\n","    visited_urls.add(section_url)\n","\n","    logging.info(f\"Visiting section URL: {section_url}\")\n","\n","    try:\n","        driver.get(section_url)\n","        time.sleep(5)  # Increase wait time to ensure the page fully loads\n","\n","        # Parse the page with BeautifulSoup\n","        page_soup = BeautifulSoup(driver.page_source, 'html.parser')\n","        content = page_soup.find('div', id='wrapper', class_='wrapper')\n","\n","        if not content:\n","            logging.error(f\"No content found on {section_url}\")\n","            return\n","\n","        # Extract the title from the <h1> tag\n","        title_tag = content.find('h1')\n","        title = title_tag.get_text(strip=True) if title_tag else 'No Title'\n","\n","        # Extract all paragraphs\n","        paragraphs = ' '.join([p.get_text(strip=True) for p in content.find_all('p')])\n","\n","        # Write the section content to the CSV\n","        csvwriter.writerow([section_url, title, paragraphs])\n","        logging.info(f\"Successfully scraped section URL: {section_url}\")\n","\n","        # Find and follow sub-links within the section\n","        sub_links = content.find_all('a', href=True)\n","        for sub_link in sub_links:\n","            sub_href = sub_link['href']\n","            if sub_href.startswith('/'):\n","                sub_full_url = \"https://www.gov.uk\" + sub_href\n","                scrape_section(sub_full_url, category_title, csvwriter, visited_urls)\n","            elif sub_href.startswith('http'):\n","                scrape_section(sub_href, category_title, csvwriter, visited_urls)\n","\n","    except Exception as e:\n","        logging.error(f\"An error occurred while scraping section {section_url}: {e}\")\n","\n","def main(url, output_csv, log_file):\n","    # Set up logging\n","    logging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","\n","    # Set up Selenium WebDriver for Firefox\n","    options = Options()\n","    options.headless = True\n","    service = Service('/opt/homebrew/bin/geckodriver')  # Use the correct path\n","    global driver\n","    driver = webdriver.Firefox(service=service, options=options)\n","\n","    try:\n","        # Open the output CSV file\n","        with open(output_csv, 'w', newline='', encoding='utf-8') as outfile:\n","            csvwriter = csv.writer(outfile)\n","            csvwriter.writerow(['Section', 'Title', 'Content'])\n","\n","            # Start scraping from the main URL\n","            visited_urls = set()\n","            scrape_category(url, 'Main Page', csvwriter, visited_urls)\n","\n","    except Exception as e:\n","        logging.error(f\"An error occurred in the main function: {e}\")\n","\n","    finally:\n","        driver.quit()\n","\n","# Example usage\n","url = \"https://www.gov.uk/browse/visas-immigration\"\n","output_csv = \"gov_additional_content.csv\"\n","log_file = \"scrape_errors.log\"\n","main(url, output_csv, log_file)"]},{"cell_type":"markdown","source":["## Files\n","\n","*   `gov_additional_content.csv`: The output CSV file where the scraped data is stored.\n","*   `scrape_errors.log`: A log file that records any errors or informational messages encountered during the scraping process.\n","\n","## Functions\n","\n","### `scrape_category(category_url, category_title, csvwriter, visited_urls)`\n","\n","This function scrapes a given category URL from the website.\n","\n","**Parameters:**\n","\n","*   `category_url` (str): The URL of the category to scrape.\n","*   `category_title` (str): The title of the category.\n","*   `csvwriter` (csv.writer): The CSV writer object.\n","*   `visited_urls` (set): A set to keep track of visited URLs to avoid re-scraping.\n","\n","**Functionality:**\n","\n","1.  **Checks for Duplicates:** Ensures the URL hasn't been visited before.\n","2.  **Navigates:** Uses Selenium to open the URL.\n","3.  **Parses HTML:** Utilizes BeautifulSoup to parse the HTML content.\n","4.  **Extracts Content:** Finds all `<p>` (paragraph) tags and extracts their text.\n","5.  **Writes to CSV:** Writes the category URL, title, and extracted text to the CSV file.\n","6.  **Finds Sub-links:** Looks for `<a>` (anchor) tags to find links.\n","7.  **Recursively Calls** `scrape_section` on the identified sub-links.\n","8. **Error Handling:** If there is any issue, it will log it in the log file.\n","\n","### `scrape_section(section_url, category_title, csvwriter, visited_urls)`\n","\n","This function scrapes a given section URL from the website.\n","\n","**Parameters:**\n","\n","*   `section_url` (str): The URL of the section to scrape.\n","*   `category_title` (str): The title of the parent category.\n","*   `csvwriter` (csv.writer): The CSV writer object.\n","*   `visited_urls` (set): A set to keep track of visited URLs to avoid re-scraping.\n","\n","**Functionality:**\n","\n","1.  **Checks for Duplicates:** Ensures the URL hasn't been visited before.\n","2.  **Navigates:** Uses Selenium to open the URL.\n","3.  **Parses HTML:** Utilizes BeautifulSoup to parse the HTML content.\n","4.  **Extracts Content:** Finds the `<h1>` tag for the title and all `<p>` tags for text.\n","5.  **Writes to CSV:** Writes the section URL, title, and extracted text to the CSV file.\n","6.  **Finds Sub-links:** Looks for `<a>` (anchor) tags to find links.\n","7.  **Recursively Calls** `scrape_section` on the identified sub-links.\n","8. **Error Handling:** If there is any issue, it will log it in the log file.\n","\n","### `main(url, output_csv, log_file)`\n","\n","This is the main function that orchestrates the scraping process.\n","\n","**Parameters:**\n","\n","*   `url` (str): The starting URL for scraping.\n","*   `output_csv` (str): The file path for the output CSV file.\n","*   `log_file` (str): The file path for the log file.\n","\n","**Functionality:**\n","\n","1.  **Logging Setup:** Configures the logging system.\n","2.  **Selenium Setup:** Initializes the Selenium WebDriver for Firefox in headless mode.\n","    *   Ensure that `geckodriver` is installed and its path is correctly specified in the code. You can use `!wget https://github.com/mozilla/geckodriver/releases/download/v0.33.0/geckodriver-v0.33.0-linux64.tar.gz` to download it, and then `!tar -xvzf geckodriver*` to unpack it.\n","3.  **CSV Setup:** Opens the specified CSV file for writing.\n","4.  **Initial Scraping:** Calls `scrape_category` to start scraping from the main URL.\n","5.  **Driver Cleanup:** Ensures the WebDriver is closed properly.\n","6. **Error Handling:** If there is any issue, it will log it in the log file.\n","\n","## Usage\n","\n","1.  **Run in Google Colab:** Execute the provided code in a Google Colab notebook.\n","2.  **Set Parameters:** Modify the `url`, `output_csv`, and `log_file` variables at the end of the script to change the starting URL, output file name, and log file name, respectively.\n","3. **Set geckodriver:** Make sure that the variable `service` in the main function points to the correct geckodriver path.\n","4.  **Execute:** Run the script to start the scraping process."],"metadata":{"id":"E7E-pfd0lRqe"}}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}